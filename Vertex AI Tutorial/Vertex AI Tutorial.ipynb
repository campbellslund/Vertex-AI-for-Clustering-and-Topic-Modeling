{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8e79021",
   "metadata": {},
   "source": [
    "# Tutorial: Using The Google Cloud Vertex AI for Clustering and Topic Modeling\n",
    "### Author: Campbell Lund\n",
    "### 10/12/2023\n",
    "This notebook walks through how to get started using the Google Cloud Vertex AI to generate text embeddings that retain sentence context. We then use these embeddings to cluster similar sentences and preform topic modeling to determine their subject. Finally, we use a text generation model to create cluster labels.\n",
    "\n",
    "### Table of contents:\n",
    "- 1. [Initialization and background](#sec1)\n",
    "- 2. [Generate embeddings](#sec2)\n",
    "- 3. [Interpreting the embeddings](#sec3)\n",
    "- 4. [Labeling the clusters ](#sec4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab978ca",
   "metadata": {},
   "source": [
    "## 1. Initialization and background<a name=\"sec1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e99a6f2",
   "metadata": {},
   "source": [
    "Vertex AI has two pre-trained models that we'll be utilizing: `textembedding-gecko@001` and `text-bison@001`.\n",
    "- The gecko model is used to create text embeddings. Simply put, text embeddings are numerical, vector representations of text. These embeddings capture semantic information about words, phrases, or documents in a way that preserves their contextual relationships. We use these vectors later for determining sentence similarity.\n",
    "- The bison model is used for text generation. Similar to ChatGPT, text-bison takes a prompt as input and returns the AI-generated response. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2b62aa",
   "metadata": {},
   "source": [
    "Import or `!pip install` the following libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb73939a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import base64\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mplcursors\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2968d3",
   "metadata": {},
   "source": [
    "In order to use the Vertex AI you'll need to create unique credentials, which will be stored in a `.json` file. `key_path` refers to the location of this file, and `PROJECT_ID` refers to the project ID created in your Google Cloud Account. For a tutorial on how to create your credentials click [here](https://learn.deeplearning.ai/google-cloud-vertex-ai/lesson/8/optional---google-cloud-setup) (note: you may need to create an account to access the tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ca5c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.service_account import Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdb2b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_path = # path to your key \n",
    "PROJECT_ID = # your project ID \n",
    "REGION = 'us-central1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1b3c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create credentials object\n",
    "credentials = Credentials.from_service_account_file(\n",
    "    key_path,\n",
    "    scopes=['https://www.googleapis.com/auth/cloud-platform'])\n",
    "\n",
    "if credentials.expired:\n",
    "    credentials.refresh(Request())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9048f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.language_models import TextGenerationModel\n",
    "from vertexai.language_models import TextEmbeddingModel\n",
    "# initialize vertex\n",
    "vertexai.init(project = PROJECT_ID, location = REGION, credentials = credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18131fb",
   "metadata": {},
   "source": [
    "### helper functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eebe3c8",
   "metadata": {},
   "source": [
    "The following helper functions were loaded from a [DeepLearning.AI tutorial](https://learn.deeplearning.ai/google-cloud-vertex-ai/lesson/1/introduction). \n",
    "- `encode_texts_to_embeddings()` takes a single string as input and returns the corresponding embeddings.\n",
    "- `encode_text_to_embedding_batched()` helps us prompt the text embedding model in batches for larger tasks. We must work in batches to avoid overloading the model and hitting rate limits. It takes a Python list of strings as input and returns a list of the corresponding embeddings. \n",
    "- `generate_batches()` creates batches of size 5 for the `encode_text_to_embedding_batched()` function. Five is the maximum batch size for the `textembedding-gecko@001` model.\n",
    "- `clusters_2D()` is a function to help us visualize the high-dimensional data on a 2D plot.\n",
    "\n",
    "It's not necessary to understand the inner workings of these functions, just how to utilize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9961eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.service_account import Credentials\n",
    "import functools\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm.auto import tqdm\n",
    "import math\n",
    "\n",
    "def generate_batches(sentences, batch_size = 5):\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        yield sentences[i : i + batch_size]\n",
    "\n",
    "def encode_texts_to_embeddings(sentences):\n",
    "    model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@001\")\n",
    "    try:\n",
    "        embeddings = model.get_embeddings(sentences)\n",
    "        return [embedding.values for embedding in embeddings]\n",
    "    except Exception:\n",
    "        return [None for _ in range(len(sentences))]\n",
    "        \n",
    "def encode_text_to_embedding_batched(sentences, api_calls_per_second = 0.33, batch_size = 5):\n",
    "    # Generates batches and calls embedding API\n",
    "    \n",
    "    embeddings_list = []\n",
    "\n",
    "    # Prepare the batches using a generator\n",
    "    batches = generate_batches(sentences, batch_size)\n",
    "\n",
    "    seconds_per_job = 1 / api_calls_per_second\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for batch in tqdm(\n",
    "            batches, total = math.ceil(len(sentences) / batch_size), position=0\n",
    "        ):\n",
    "            futures.append(\n",
    "                executor.submit(functools.partial(encode_texts_to_embeddings), batch)\n",
    "            )\n",
    "            time.sleep(seconds_per_job)\n",
    "\n",
    "        for future in futures:\n",
    "            embeddings_list.extend(future.result())\n",
    "\n",
    "    is_successful = [\n",
    "        embedding is not None for sentence, embedding in zip(sentences, embeddings_list)\n",
    "    ]\n",
    "    embeddings_list_successful = np.squeeze(\n",
    "        np.stack([embedding for embedding in embeddings_list if embedding is not None])\n",
    "    )\n",
    "    return is_successful, embeddings_list_successful\n",
    "\n",
    "def clusters_2D(x_values, y_values, labels, kmeans_labels):\n",
    "    fig, ax = plt.subplots()\n",
    "    scatter = ax.scatter(x_values, \n",
    "                         y_values, \n",
    "                         c = kmeans_labels, \n",
    "                         cmap='Set1', \n",
    "                         alpha=0.5, \n",
    "                         edgecolors='k', \n",
    "                         s = 40)  # Change the denominator as per n_clusters\n",
    "\n",
    "    # Create a mplcursors object to manage the data point interaction\n",
    "    cursor = mplcursors.cursor(scatter, hover=True)\n",
    "\n",
    "    #axes\n",
    "    ax.set_title('Embedding clusters visualization in 2D')  # Add a title\n",
    "    ax.set_xlabel('X_1')  # Add x-axis label\n",
    "    ax.set_ylabel('X_2')  # Add y-axis label\n",
    "\n",
    "    # Define how each annotation should look\n",
    "    @cursor.connect(\"add\")\n",
    "    def on_add(sel):\n",
    "        sel.annotation.set_text(labels.category[sel.target.index])\n",
    "        sel.annotation.get_bbox_patch().set(facecolor='white', alpha=0.95) # Set annotation's background color\n",
    "        sel.annotation.set_fontsize(14) \n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69fae62",
   "metadata": {},
   "source": [
    "### read the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7616c626",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/allQueries.csv', header=None, names=[\"sentences\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45eb74f",
   "metadata": {},
   "source": [
    "The following `df` contains the 628 sentences that we'll create text embeddings for in this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9a6de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9c8936",
   "metadata": {},
   "source": [
    "## 2. Generate embeddings<a name=\"sec2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8efc86",
   "metadata": {},
   "source": [
    "Only run the following cells if you're making embeddings for your own data as it takes some time to compile. If you're following along with the tutorial jump to section 3 to use the saved embeddings in `sentence_embeddings.pkl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55f8107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert our df to a list\n",
    "sentence_list = df.sentences.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1989dfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the encode_text_to_embedding_batched() helper function to generate embeddings\n",
    "is_successful, sentence_embeddings = encode_text_to_embedding_batched(\n",
    "                            sentences=sentence_list,\n",
    "                            api_calls_per_second = 20/60, \n",
    "                            batch_size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4a533e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7674d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for successfully embedded sentences\n",
    "sentence_list = np.array(sentence_list)[is_successful]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bbebf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write embeddings to a pickle file\n",
    "with open('data/sentence_embeddings.pkl', 'wb') as file:\n",
    "    pickle.dump(sentence_embeddings, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06528801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the successfully embedded sentence list to a csv file\n",
    "with open('data/filtered_sentences.pkl', 'wb') as file:\n",
    "    pickle.dump(sentence_list, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76ce9fc",
   "metadata": {},
   "source": [
    "## 3. Interpreting the embeddings<a name=\"sec3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18901caa",
   "metadata": {},
   "source": [
    "### read the saved data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0720c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/sentence_embeddings.pkl', 'rb') as file:\n",
    "    sentence_embeddings = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1620e860",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/filtered_sentences.pkl', 'rb') as file:\n",
    "    sentence_list = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02729a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7532ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f79365",
   "metadata": {},
   "source": [
    "### clustering:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971d098e",
   "metadata": {},
   "source": [
    "To group the sentences by similarity, we'll use KMeans clustering. This is a common machine learning algorithm that works to determine patterns and commonalities between data. Here, we fit the model to our sentence embeddings and ask it to divide the corresponding sentences into `k` distinct groups or “clusters”. \n",
    "\n",
    "Again, don't worry about fully understanding this section if you haven't taken a machine learning class before - what's important is the number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9573320",
   "metadata": {},
   "source": [
    "Import or `!pip install` the following libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c62bb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77791579",
   "metadata": {},
   "source": [
    "Try playing around with changing the number of `n_clusters`. You'll notice that the cluster topics identified in the following section will consist of multiple different topics if `n_clusters` is too low, but might lose integrity if it's too high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecff6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this variable determines the number of clusters\n",
    "n_clusters = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d715f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the KMeans algorithm\n",
    "kmeans = KMeans(n_clusters=n_clusters, \n",
    "                random_state=0, \n",
    "                n_init = 'auto').fit(sentence_embeddings)\n",
    "\n",
    "kmeans_labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39f5af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the dimensionality of the data to help us visualize and interpret it better\n",
    "PCA_model = PCA(n_components=2)\n",
    "PCA_model.fit(sentence_embeddings)\n",
    "new_values = PCA_model.transform(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28079645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use our helper function to display the clusters in 2D\n",
    "clusters_2D(x_values = new_values[:,0], y_values = new_values[:,1], \n",
    "            labels = df, kmeans_labels = kmeans_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7cc39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = [[] for cluster in range(n_clusters)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad548c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the sentences into lists based on their clusters\n",
    "for i in range(len(sentence_list)):\n",
    "    cluster_index = kmeans_labels[i]\n",
    "    clusters[cluster_index].append(sentence_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13baf969",
   "metadata": {},
   "source": [
    "### topic modeling:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e894cdc7",
   "metadata": {},
   "source": [
    "Now that we've seperated the sentences into distinct groups, we can use topic modeling to determine the subject of each cluster. Topic modeling is a common Natural Language Processing technique - we'll be using the Latent Dirichlet Allocation (LDA) algorithm in our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1adb42",
   "metadata": {},
   "source": [
    "Import or `!pip install` the following libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d30622",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "from gensim.models import CoherenceModel\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eace13bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for cleaning and tokenizing the sentences\n",
    "def clean_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    punctuation = set(string.punctuation)\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [token for token in tokens if token not in stop_words and token not in punctuation]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f44c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_topics = []\n",
    "all_vis_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983eee53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train LDA model for each cluster\n",
    "for i, cluster in enumerate(clusters):\n",
    "    # clean the text\n",
    "    cluster_doc = [' '.join(cluster)]\n",
    "    processed_docs = [clean_text(doc) for doc in cluster_doc]\n",
    "\n",
    "    # create a dictionary and corpus\n",
    "    dictionary = corpora.Dictionary(processed_docs)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "\n",
    "    # train model\n",
    "    num_topics = 3  \n",
    "    lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15)\n",
    "\n",
    "    # print the results\n",
    "    topics = \"\"\n",
    "    print(f\"Cluster {i + 1} Topics:\")\n",
    "    for topic_num, words in lda_model.print_topics():\n",
    "        print(f\"Topic {topic_num + 1}: {words}\")\n",
    "        topics += words\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # save topic and visualization data for later\n",
    "    all_topics.append(topics)\n",
    "    vis_data = gensimvis.prepare(lda_model, corpus, dictionary)\n",
    "    all_vis_data.append(vis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a38431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the topics - change the index of all_vis_data to view a different cluster\n",
    "pyLDAvis.display(all_vis_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e4b331",
   "metadata": {},
   "source": [
    "### manually audit what's in each cluster:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ee969b",
   "metadata": {},
   "source": [
    "To verify how well our clustering and topic modeling algorithms preformed we can print the contents of each cluster for cross-referencing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4216acb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, cluster in enumerate(clusters):\n",
    "    print(f\"Cluster {i + 1}: \")\n",
    "    print(cluster)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6969b4",
   "metadata": {},
   "source": [
    "## 4. Labeling the clusters<a name=\"sec4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edf92df",
   "metadata": {},
   "source": [
    "Now that we're satisfied with the number of clusters and their contents, let's create a label for each of them. We'll use a text generation model to achieve this, starting with Vertex AI's `text-bison@001`. Text-bison takes a prompt as a string and returns the response of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19021f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_model = TextGenerationModel.from_pretrained(\"text-bison@001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bcef9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, topic in enumerate(all_topics):\n",
    "    prompt = f'''Your job is to create labels for n={n_clusters} clusters. \\\n",
    "    Given the topics with their associated weights, output a single, master topic \\\n",
    "    that summarizes all the topics identified in the cluster.\\\n",
    "    Topics: {topic} .'''\n",
    "    print(f\"Cluster: {i+1} Topic: {generation_model.predict(prompt=prompt).text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc736784",
   "metadata": {},
   "source": [
    "As you can see this result isn't as succinct as we want. Feel free to edit the prompt to try and improve the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcb0c5b",
   "metadata": {},
   "source": [
    "### switching models:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771d6353",
   "metadata": {},
   "source": [
    "Since text-bison isn't doing a very good job preforming the task we've instructed it to do, lets try switching models to `gpt-3.5-turbo`. If you haven't worked with the OpenAI API before check out my tutorial [here](https://github.com/campbellslund/OpenAI-API-for-Categorization-and-Labeling/blob/main/OpenAI%20API%20Tutorial/Using%20The%20OpenAI%20API%20for%20Categorization%20and%20Labeling%20Tutorial.ipynb) for step-by-step instructions getting started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9473606d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82a35cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieving our API key from a secure file\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai.api_key  = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93d1ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the model's response to a given message query\n",
    "def get_completion_from_messages(messages, \n",
    "                                 model=\"gpt-3.5-turbo\", \n",
    "                                 temperature=0, # degree of randomness\n",
    "                                 max_tokens=150): #4000 is max for input and response combined\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, \n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bafb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "delimiter = \"####\"\n",
    "system_message = f\"\"\"your job is to create labels for clusters. \\\n",
    "    Given the topic modeling data of topics with their associated weights, \\\n",
    "    output a single, master topic that summarizes all the topics identified in each cluster.\\\n",
    "    Clusters will be seperated by {delimiter} characters.\"\"\"\n",
    "\n",
    "user_message = f\"\"\"{delimiter}\"\"\"\n",
    "for i, topic in enumerate(all_topics):\n",
    "    user_message += f\"\"\"{topic}{delimiter}\"\"\"\n",
    "    \n",
    "messages =  [  \n",
    "{'role':'system', \n",
    " 'content': system_message},    \n",
    "{'role':'user', \n",
    " 'content': user_message},  \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f440c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_completion_from_messages(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b97f933",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c612130a",
   "metadata": {},
   "source": [
    "Much better! Now we have labels for our clusters. Again, make sure to verify these with the actual content of the clusters - it's good practice to always have a human in the loop auditing the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
